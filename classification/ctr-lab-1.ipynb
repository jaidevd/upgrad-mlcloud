{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://brainiac.Dlink:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9ca164b390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+----+------+---+---------+---+----+\n",
      "|click|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18| C19|   C20|C21|dayofweek|day|hour|\n",
      "+-----+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+----+------+---+---------+---+----+\n",
      "|false|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| c2085c57|    c6263d8a|          1|               0|15699|320| 50|1722|  0|  35|    -1| 79|        1| 21|   6|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|3f2a6cbb|  33da2e74|    cef3e649| a99f214a| af62faf4|    03683bd4|          1|               0|19733|320| 50|2260|  0| 171|100156| 91|        2| 22|   9|\n",
      "|false|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| cc8384f2|    76dc4769|          1|               0|15703|320| 50|1722|  0|  35|    -1| 79|        4| 24|  20|\n",
      "|false|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 37b3fd2c|    dab823eb|          1|               0|15705|320| 50|1722|  0|  35|    -1| 79|        2| 29|  11|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|03528b27|  2347f47a|    8ded1f7a| a99f214a| 9b913bd2|    6715887e|          1|               0| 6563|320| 50| 572|  2|  39|    -1| 32|        1| 21|  15|\n",
      "|false|1005|         2|424f0ecf|   8a704386|     f66779e6|ecad2386|  7801e8d9|    07d7df22| a99f214a| b25c0aa7|    39112ea2|          1|               0|19998|216| 36|2281|  3|  47|100182| 42|        3| 23|  10|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|92f5800b|  ae637522|    0f2161f8| a99f214a| f9089e25|    981edffc|          1|               3|21191|320| 50|2424|  1| 161|    -1| 71|        1| 28|   7|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|e2fcccd2|  5c5a694b|    0f2161f8| 99d5e09b| f94f4d25|    be74e6fe|          1|               0| 4687|320| 50| 423|  2|  39|100148| 32|        3| 23|  13|\n",
      "| true|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| d9f95ce3|    711ee120|          1|               0|20108|320| 50|2299|  2|1327|100084| 52|        5| 25|  15|\n",
      "| true|1005|         0|5b08c53b|   7687a86e|     3e814130|ecad2386|  7801e8d9|    07d7df22| a99f214a| c0ee5db8|    375c3d47|          1|               0|17654|300|250|1994|  2|  39|100084| 33|        6| 26|   6|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|39947756|  2347f47a|    cef3e649| 77f8ae88| aefe6349|    ecb851b2|          1|               3|21767|320| 50|2506|  0|  35|100020|157|        2| 22|  12|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|03528b27|  2347f47a|    8ded1f7a| a99f214a| ebd37151|    5096d134|          1|               0|16688|320| 50|1873|  3|  39|    -1| 23|        0| 27|  16|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|9c13b419|  2347f47a|    f95efa07| a99f214a| 2e20b339|    737e5d8b|          1|               0|18094|320| 50|2060|  3|  39|    -1| 23|        2| 22|  19|\n",
      "|false|1005|         1|e151e245|   7e091613|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| f5527a54|    80b1a1d2|          1|               0|17014|320| 50|1872|  3|  39|    -1| 23|        6| 26|  16|\n",
      "|false|1005|         1|e151e245|   7e091613|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| 3763df1d|    5db079b5|          1|               2|21687|320| 50|2495|  2| 167|    -1| 23|        5| 25|   6|\n",
      "|false|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 1483d8fe|    1f0bc64f|          1|               0|15706|320| 50|1722|  0|  35|    -1| 79|        1| 21|   4|\n",
      "|false|1005|         1|57ef2c87|   bd6d812f|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| 17abe1bc|    8967e320|          1|               0|19771|320| 50|2227|  0| 679|100081| 48|        4| 24|  14|\n",
      "|false|1005|         0|85f751fd|   c4e18dd6|     50e219e0|685d1c4c|  2347f47a|    8ded1f7a| 3c696fca| 8fe536b7|    8ce34352|          1|               3|15705|320| 50|1722|  0|  35|    -1| 79|        3| 23|  13|\n",
      "|false|1005|         1|5b4d2eda|   16a36ef3|     f028772b|ecad2386|  7801e8d9|    07d7df22| a99f214a| 1bd35886|    2d854941|          1|               0|20251|320| 50|2323|  0| 687|100081| 48|        6| 26|   6|\n",
      "| true|1005|         0|5b08c53b|   7687a86e|     3e814130|ecad2386|  7801e8d9|    07d7df22| a99f214a| 87895f8a|    8a4875bd|          1|               0|20345|300|250|2331|  2|  39|    -1| 23|        0| 27|   7|\n",
      "+-----+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+----+------+---+---------+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv('../data/avazu/1M.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = ['C1'] + [f'C{k}' for k in range(14, 22)]\n",
    "catCols += ['banner_pos', 'site_category', 'device_type', 'device_conn_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+---+----+---+----+------+---+----------+-------------+-----------+----------------+\n",
      "|  C1|  C14|C15|C16| C17|C18| C19|   C20|C21|banner_pos|site_category|device_type|device_conn_type|\n",
      "+----+-----+---+---+----+---+----+------+---+----------+-------------+-----------+----------------+\n",
      "|1005|15699|320| 50|1722|  0|  35|    -1| 79|         0|     28905ebd|          1|               0|\n",
      "|1005|19733|320| 50|2260|  0| 171|100156| 91|         0|     50e219e0|          1|               0|\n",
      "|1005|15703|320| 50|1722|  0|  35|    -1| 79|         0|     28905ebd|          1|               0|\n",
      "|1005|15705|320| 50|1722|  0|  35|    -1| 79|         0|     28905ebd|          1|               0|\n",
      "|1005| 6563|320| 50| 572|  2|  39|    -1| 32|         0|     50e219e0|          1|               0|\n",
      "|1005|19998|216| 36|2281|  3|  47|100182| 42|         2|     f66779e6|          1|               0|\n",
      "|1005|21191|320| 50|2424|  1| 161|    -1| 71|         0|     50e219e0|          1|               3|\n",
      "|1005| 4687|320| 50| 423|  2|  39|100148| 32|         0|     50e219e0|          1|               0|\n",
      "|1005|20108|320| 50|2299|  2|1327|100084| 52|         0|     28905ebd|          1|               0|\n",
      "|1005|17654|300|250|1994|  2|  39|100084| 33|         0|     3e814130|          1|               0|\n",
      "|1005|21767|320| 50|2506|  0|  35|100020|157|         0|     50e219e0|          1|               3|\n",
      "|1005|16688|320| 50|1873|  3|  39|    -1| 23|         0|     50e219e0|          1|               0|\n",
      "|1005|18094|320| 50|2060|  3|  39|    -1| 23|         0|     50e219e0|          1|               0|\n",
      "|1005|17014|320| 50|1872|  3|  39|    -1| 23|         1|     f028772b|          1|               0|\n",
      "|1005|21687|320| 50|2495|  2| 167|    -1| 23|         1|     f028772b|          1|               2|\n",
      "|1005|15706|320| 50|1722|  0|  35|    -1| 79|         0|     28905ebd|          1|               0|\n",
      "|1005|19771|320| 50|2227|  0| 679|100081| 48|         1|     f028772b|          1|               0|\n",
      "|1005|15705|320| 50|1722|  0|  35|    -1| 79|         0|     50e219e0|          1|               3|\n",
      "|1005|20251|320| 50|2323|  0| 687|100081| 48|         1|     f028772b|          1|               0|\n",
      "|1005|20345|300|250|2331|  2|  39|    -1| 23|         0|     3e814130|          1|               0|\n",
      "+----+-----+---+---+----+---+----+------+---+----------+-------------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(catCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "posMapper = udf(lambda x: 0 if x < 0 else x)\n",
    "df = df.withColumn('C20_1', posMapper(df['C20']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "si = StringIndexer(inputCol='site_category', outputCol='site_category_ix')\n",
    "df = si.fit(df).transform(df)\n",
    "catCols.remove('site_category')\n",
    "catCols.remove('C20')\n",
    "catCols.append('site_category_ix')\n",
    "catCols.append('C20_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| C20_1|\n",
      "+------+\n",
      "|     0|\n",
      "|100156|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|100182|\n",
      "|     0|\n",
      "|100148|\n",
      "|100084|\n",
      "|100084|\n",
      "|100020|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|     0|\n",
      "|100081|\n",
      "|     0|\n",
      "|100081|\n",
      "|     0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('C20_1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"C20_1int\", df['C20_1'].cast(IntegerType()))\n",
    "catCols.remove('C20_1')\n",
    "catCols.append('C20_1int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoderEstimator(inputCols=catCols, outputCols=[c + 'Enc' for c in catCols])\n",
    "enc_model = encoder.fit(df)\n",
    "encoded = enc_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(100248, {0: 1.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.select('C20_1intEnc').head().C20_1intEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['click',\n",
       " 'C1',\n",
       " 'banner_pos',\n",
       " 'site_id',\n",
       " 'site_domain',\n",
       " 'site_category',\n",
       " 'app_id',\n",
       " 'app_domain',\n",
       " 'app_category',\n",
       " 'device_id',\n",
       " 'device_ip',\n",
       " 'device_model',\n",
       " 'device_type',\n",
       " 'device_conn_type',\n",
       " 'C14',\n",
       " 'C15',\n",
       " 'C16',\n",
       " 'C17',\n",
       " 'C18',\n",
       " 'C19',\n",
       " 'C20',\n",
       " 'C21',\n",
       " 'dayofweek',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'C20_1',\n",
       " 'site_category_ix',\n",
       " 'C20_1int',\n",
       " 'C15Enc',\n",
       " 'C21Enc',\n",
       " 'C17Enc',\n",
       " 'device_conn_typeEnc',\n",
       " 'C19Enc',\n",
       " 'C14Enc',\n",
       " 'site_category_ixEnc',\n",
       " 'device_typeEnc',\n",
       " 'C1Enc',\n",
       " 'C16Enc',\n",
       " 'C18Enc',\n",
       " 'banner_posEnc',\n",
       " 'C20_1intEnc']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCols = [c for c in encoded.columns if c.endswith('Enc')] + ['day', 'hour', 'dayofweek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded.withColumn('label', encoded['click'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.select(*trainCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mtoString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not convert %s to string type\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9fc654bddda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, threshold, thresholds, probabilityCol, rawPredictionCol, standardization, weightCol, aggregationDepth, family, lowerBoundsOnCoefficients, upperBoundsOnCoefficients, lowerBoundsOnIntercepts, upperBoundsOnIntercepts)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1E-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkThresholdConsistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/classification.py\u001b[0m in \u001b[0;36msetParams\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, threshold, thresholds, probabilityCol, rawPredictionCol, standardization, weightCol, aggregationDepth, family, lowerBoundsOnCoefficients, upperBoundsOnCoefficients, lowerBoundsOnIntercepts, upperBoundsOnIntercepts)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkThresholdConsistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol=trainCols, labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C15Enc',\n",
       " 'C21Enc',\n",
       " 'C17Enc',\n",
       " 'device_conn_typeEnc',\n",
       " 'C19Enc',\n",
       " 'C14Enc',\n",
       " 'site_category_ixEnc',\n",
       " 'device_typeEnc',\n",
       " 'C1Enc',\n",
       " 'C16Enc',\n",
       " 'C18Enc',\n",
       " 'banner_posEnc',\n",
       " 'C20_1intEnc',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'dayofweek']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=trainCols, outputCol='features')\n",
    "encoded = va.transform(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jaidevd/src/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/jaidevd/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o712.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 59.0 failed 1 times, most recent failure: Lost task 3.0 in stage 59.0 (TID 1912, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3898)\n",
      "\tat java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3705)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2115)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:23)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:55)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:48)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:89)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:803)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3898)\n",
      "\tat java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3705)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2115)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jaidevd/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jaidevd/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/jaidevd/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/src/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o712.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 59.0 failed 1 times, most recent failure: Lost task 3.0 in stage 59.0 (TID 1912, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3898)\n\tat java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3705)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2115)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:23)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:55)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:48)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:89)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:803)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:3898)\n\tat java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3705)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2115)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-7018a895436c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             stackTrace = '\\n\\t at '.join(map(lambda x: x.toString(),\n\u001b[0;32m---> 67\u001b[0;31m                                              e.java_exception.getStackTrace()))\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__compute_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             raise TypeError(\"array indices must be integers, not {0}\".format(\n",
      "\u001b[0;32m~/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\u001b[0m in \u001b[0;36m__compute_item\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 59860)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jaidevd/anaconda3/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/jaidevd/anaconda3/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/jaidevd/anaconda3/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/jaidevd/anaconda3/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/jaidevd/src/spark/python/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/jaidevd/src/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/home/jaidevd/src/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/jaidevd/src/spark/python/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lr.fit(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
